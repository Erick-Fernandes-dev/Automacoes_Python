# Documenta√ß√£o de scripts Python

## Web Scraping

## Objetivo do Projeto

Desenvolvi uma solu√ß√£o automatizada para acessar o portal da Ag√™ncia Nacional de Sa√∫de Suplementar (ANS) no dom√≠nio [www.gov.br](http://www.gov.br/), especificamente na se√ß√£o de **Atualiza√ß√£o do Rol de Procedimentos**, com as seguintes funcionalidades:

1. **Acesso automatizado** ao portal oficial
2. **Download seguro** dos anexos I e II em formato PDF
3. **Processamento e compacta√ß√£o** dos documentos

## Implementa√ß√£o T√©cnica

O sistema foi desenvolvido em Python utilizando as seguintes tecnologias:

- **Selenium**
- **loguru**
- **os**
- **urllib**
- **datetime**

```python
pip install Selenium loguru urllib datetime
```

### Script do Projeto:

```python
from selenium import webdriver as wb
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from loguru import logger as log
import os
from urllib.request import urlretrieve
import zipfile
from datetime import datetime as dt

class WebScraper:
    def __init__(self, url):
        self.url = url
        self.driver = None
        
    
    # Essa fun√ß√£o aqui vai iniciar o navegador
    # e acessar a URL desejada
    def start_browser(self):
        try:
            log.info("Iniciando o navegador")
            options = wb.FirefoxOptions()
            options.set_preference("browser.download.folderList", 2)
            options.set_preference("browser.download.dir", "/home/wolf/Downloads")
            options.set_preference("browser.helperApps.neverAsk.saveToDisk", "application/pdf")
            
            log.info("Iniciando o navegador Firefox com configura√ß√µes de download")
            self.driver = wb.Firefox(options=options)
            
            self.driver.get(self.url)
            log.info(f"Acessando a URL: {self.url}")

            #self.driver.maximize_window()
            #log.info("Maximizando a tela")
            
            #log.info(f"T√≠tulo atual: {self.driver.title}")
            #log.info(f"URL atual: {self.driver.current_url}")

            # Fazendo uma verifica√ß√£o mais robusta do t√≠tulo da p√°gina
            WebDriverWait(self.driver, 10).until(
                EC.title_contains("Atualiza√ß√£o do Rol de Procedimentos")
            )
            log.debug("T√≠tulo da p√°gina verificado com sucesso")

            log.debug("Fun√ß√£o executada com sucesso")
            #self.driver.find_element(By.XPATH, "/html/body/div[2]/div[1]/main/div[2]/div/div/div/div/div[2]/div/ol/li[1]/a[1]").click()

        except Exception as e:
            log.error(f"Erro ao iniciar o navegador: {str(e)}")
            if self.driver:
                self.driver.quit()
            return False

    def get_anexos(self, download_dir="downloads"):
        """Baixa os anexos usando urllib (biblioteca padr√£o)"""
        try:
            os.makedirs(download_dir, exist_ok=True)
            
            # Localiza os links dos anexos
            anexo_i = WebDriverWait(self.driver, 15).until(
                EC.presence_of_element_located((By.XPATH, "//a[contains(., 'Anexo I') and contains(@href, '.pdf')]"))
            )
            anexo_ii = WebDriverWait(self.driver, 15).until(
                EC.presence_of_element_located((By.XPATH, "//a[contains(., 'Anexo II') and contains(@href, '.pdf')]"))
            )

            # Obt√©m URLs
            url_i = anexo_i.get_attribute('href')
            url_ii = anexo_ii.get_attribute('href')

            # Faz o download
            urlretrieve(url_i, os.path.join(download_dir, "Anexo_I.pdf"))
            urlretrieve(url_ii, os.path.join(download_dir, "Anexo_II.pdf"))
            
            return True

        except Exception as e:
            log.error(f"Erro ao baixar anexos: {str(e)}")
            return False

    
    def close_browser(self):
        log.info("Fechando o navegador")
        if self.driver:
            self.driver.close()
            log.debug("Navegador fechado com sucesso")

    # Fun√ß√£o que vaI ZIPAR os anexos
    def zip_anexos(self, download_dir="downloads", output_dir="output"):

        try:
            # Cria diret√≥rio de sa√≠da se n√£o existir
            os.makedirs(output_dir, exist_ok=True)
            
            # Verifica se os anexos existem
            anexo_i_path = os.path.join(download_dir, "Anexo_I.pdf")
            anexo_ii_path = os.path.join(download_dir, "Anexo_II.pdf")
            
            if not all([os.path.exists(anexo_i_path), os.path.exists(anexo_ii_path)]):
                log.error("Arquivos dos anexos n√£o encontrados para compacta√ß√£o")
                return None
            
            # Nome do arquivo ZIP com timestamp
            timestamp = dt.now().strftime("%Y%m%d_%H%M%S")
            zip_filename = f"Teste_ErickFernandesDeFariasSantos_{timestamp}.zip"
            zip_path = os.path.join(output_dir, zip_filename)
            
            # Cria o arquivo ZIP
            log.info(f"Criando arquivo ZIP: {zip_path}")
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                zipf.write(anexo_i_path, arcname="Anexo_I.pdf")
                zipf.write(anexo_ii_path, arcname="Anexo_II.pdf")
                log.success(f"Adicionados Anexo_I.pdf e Anexo_II.pdf ao ZIP")
            
            log.success(f"Compacta√ß√£o conclu√≠da: {zip_path}")
            return zip_path
            
        except Exception as e:
            log.error(f"Erro ao compactar anexos: {str(e)}")
            return None
    

if __name__ == "__main__":
    url = "https://www.gov.br/ans/pt-br/acesso-a-informacao/participacao-da-sociedade/atualizacao-do-rol-de-procedimentos"
    scraper = WebScraper(url)

    try:
        scraper.start_browser()
        scraper.get_anexos()
        scraper.zip_anexos()
    except Exception as e:
        log.error(f"Erro durante a execu√ß√£o: {str(e)}")
    finally:
        #scraper.close_browser()
        log.info("Execu√ß√£o finalizada")

```

![image.png](/Documenta√ß√£o de scripts Python 1c51e83b35a080218439f497b3b174e4/image.png)

## Transforma√ß√£o de Dados

- Pr√≥ximo passo, vamos realizar a transforma√ß√£o dos dados pegando o arquivo do anexo I.

*O objetivo desse Script √© automatizar a extra√ß√£o, transforma√ß√£o e organiza√ß√£o de dados de planilhas contidas em arquivos PDF de regulamentos da ANS (Ag√™ncia  Nacional de Sa√∫de Suplementar), convertendo-os em formatos estruturados (CSV/ZIP) para facilitar an√°lise e integra√ß√£o com outros sistemas."*

### Detalhamento para documenta√ß√£o:

1. **Finalidade**:
    
    Transformar dados n√£o-estruturados (tabelas em PDF) em dados estruturados prontos para an√°lise.
    
2. **Funcionalidades-chave**:
    - Extra√ß√£o autom√°tica de tabelas de PDFs complexos
    - Limpeza e padroniza√ß√£o dos dados
    - Convers√£o para CSV com codifica√ß√£o UTF-8
    - Compacta√ß√£o dos resultados
3. **Diferenciais**:
    - Rastreabilidade completa do processo (logs detalhados)
    - Visualiza√ß√£o do progresso em tempo real
    - Tratamento de erros robusto
4. **Aplica√ß√µes t√≠picas**:
    - Integra√ß√£o com sistemas de sa√∫de
    - An√°lise de cobertura de procedimentos
    - Atualiza√ß√£o de bancos de dados regulat√≥rios

### O que voc√™ precisa para executar esse Script?

Execute essa linha de comando para poder instalar essas libs

```bash
pip install loguru pdfplumber pandas tqdm
```

Ou, voc√™ cria um arquivo **`.txt`** como o nome **`requirement`** dentro do diret√≥rio onde se encontra o scrip

requirement.txt

```bash
loguru 
pdfplumber
pandas
tqdm
```

Agora, vamos executar essa linha de comando:

```bash
pip install -r requirements.txt
```

### Scripts

main.py

```python
import Data_transformtion as dt
from pathlib import Path
from loguru import logger as log
import os
from tqdm import tqdm

if __name__ == "__main__":
    try:
        log.remove()
        log.add(lambda msg: tqdm.write(msg, end=""), colorize=True)
        
        project_root = Path(__file__).parent.parent
        pdf_path = project_root / "web_scraping" / "downloads" / "Anexo_I.pdf"
        
        if not pdf_path.exists():
            log.error(f"Arquivo PDF n√£o encontrado em: {pdf_path}")
            exit(1)
        
        log.info(f"Iniciando processamento do arquivo: {pdf_path}")
        transformer = dt.ANSDataTransformer(str(pdf_path))
        success = transformer.process("ErickFernandesDeFariasSantos")
        
        if success:
            log.info("Processo conclu√≠do com sucesso!")
        else:
            log.error("Ocorreu um erro durante o processamento")
    
    except Exception as e:
        log.error(f"Erro fatal: {str(e)}")
```

Data_transformtion.py

```python
import os
import pandas as pd
import pdfplumber
import zipfile
from pathlib import Path
from datetime import datetime
from loguru import logger as log
from tqdm import tqdm
from tqdm.contrib.logging import logging_redirect_tqdm
import logging

class ANSDataTransformer:
    def __init__(self, pdf_path):
        """Inicializa o transformador com o caminho do PDF"""
        self.pdf_path = str(Path(pdf_path).absolute())
        self.output_dir = str(Path(__file__).parent.parent / "output")
        self.column_mapping = {
            'OD': 'Seg. Odontol√≥gica',
            'AMB': 'Seg. Ambulatorial',
            'HCO': 'Seg. Hospitalar Com Obstetr√≠cia',
            'HSO': 'Seg. Hospitalar Sem Obstetr√≠cia',
            'REF': 'Plano Refer√™ncia',
            'PAC': 'Procedimento Alta Complexidade',
            'DUT': 'Diretriz Utiliza√ß√£o'
        }
        
        if not os.path.exists(self.pdf_path):
            available_files = "\n".join(os.listdir(Path(self.pdf_path).parent))
            raise FileNotFoundError(
                f"Arquivo PDF n√£o encontrado em {self.pdf_path}\n"
                f"Arquivos dispon√≠veis:\n{available_files}"
            )

    def extract_tables_from_pdf(self):
        """Extrai tabelas a partir da p√°gina 3 do PDF"""
        log.info(f"Iniciando extra√ß√£o de dados do PDF: {self.pdf_path}")
        all_tables = []
        
        try:
            with pdfplumber.open(self.pdf_path) as pdf:
                with logging_redirect_tqdm():
                    # Processa apenas a partir da p√°gina 3 (√≠ndice 2)
                    for page in tqdm(pdf.pages[2:], desc="Extraindo p√°ginas", unit="p√°gina"):
                        # Configura√ß√µes aprimoradas para extra√ß√£o de tabelas
                        table_settings = {
                            'vertical_strategy': 'lines',
                            'horizontal_strategy': 'lines',
                            'snap_tolerance': 5,
                            'join_tolerance': 10,
                            'edge_min_length': 15,
                            'text_x_tolerance': 2,
                            'text_y_tolerance': 2
                        }
                        
                        table = page.extract_table(table_settings)
                        
                        if table:
                            # Filtra cabe√ßalhos repetidos e linhas de legenda
                            if "PROCEDIMENTO" in table[0][0]:
                                header = table[0]
                                all_tables.append(header)
                                all_tables.extend(table[1:])
                            else:
                                all_tables.extend(table)
                            
                            # Adiciona uma linha vazia ap√≥s cada p√°gina
                            empty_row = [''] * len(table[0])
                            all_tables.append(empty_row)
            
            if len(all_tables) > 0:
                log.success(f"Extra√≠das {len(all_tables)-1} linhas de dados")
                return all_tables
            else:
                log.error("Nenhuma tabela v√°lida encontrada no PDF")
                return None
        
        except Exception as e:
            log.error(f"Erro ao extrair tabelas do PDF: {str(e)}")
            return None
    def clean_and_transform_data(self, raw_data):
        """Limpa e transforma os dados extra√≠dos"""
        log.info("Iniciando transforma√ß√£o dos dados")
        
        try:
            if len(raw_data) < 2:
                raise ValueError("Dados insuficientes para transforma√ß√£o")
            
            # Cria DataFrame mantendo o cabe√ßalho original
            df = pd.DataFrame(raw_data[1:], columns=raw_data[0])
            
            # Limpeza dos dados
            with logging_redirect_tqdm():
                # Remove linhas completamente vazias
                df = df.dropna(how='all')
                
                # Limpeza de strings e tratamento de multilinha
                for col in tqdm(df.columns, desc="Limpando colunas", leave=False):
                    df[col] = df[col].astype(str).str.strip().str.replace('\n', ' ')
                    df[col] = df[col].replace({'nan': '', 'None': ''})
                
                # Remove linhas com elementos da legenda
                df = df[~df.iloc[:, 0].str.contains('Legenda:|OD:|AMB:|HCO:|HSO:|REF:|PAC:|DUT:', na=False)]
                
                # Remove colunas vazias
                df = df.dropna(axis=1, how='all')
                
                # Renomeia colunas conforme estrutura conhecida
                df.columns = [
                    'PROCEDIMENTO', 
                    'RN_ALTERACAO', 
                    'VIGENCIA', 
                    'OD', 
                    'AMB', 
                    'HCO', 
                    'HSO', 
                    'REF', 
                    'PAC', 
                    'DUT', 
                    'SUBGRUPO', 
                    'GRUPO', 
                    'CAPITULO'
                ]
                
                # Substitui abrevia√ß√µes
                df = self._replace_abbreviations(df)
            
            log.success(f"Dados transformados com sucesso. Shape final: {df.shape}")
            return df
        
        except Exception as e:
            log.error(f"Erro ao transformar dados: {str(e)}")
            return None

    def _replace_abbreviations(self, df):
        """Substitui as abrevia√ß√µes conforme a legenda"""
        for col in self.column_mapping.keys():
            if col in df.columns:
                df[col] = df[col].apply(
                    lambda x: self.column_mapping[col] if str(x).strip() == 'X' else ''
                )
        return df

    def save_to_csv(self, dataframe, filename="rol_procedimentos.csv"):
        """Salva os dados em arquivo CSV"""
        try:
            os.makedirs(self.output_dir, exist_ok=True)
            csv_path = os.path.join(self.output_dir, filename)
            
            with logging_redirect_tqdm():
                with tqdm(total=100, desc="Salvando CSV") as pbar:
                    dataframe.to_csv(csv_path, index=False, encoding='utf-8-sig', sep=';')
                    pbar.update(100)
            
            log.success(f"CSV salvo em: {csv_path}")
            return csv_path
        except Exception as e:
            log.error(f"Erro ao salvar CSV: {str(e)}")
            return None

    def compress_csv(self, csv_path, your_name):
        """Compacta o arquivo CSV em um ZIP"""
        try:
            if not os.path.exists(csv_path):
                raise FileNotFoundError(f"Arquivo CSV n√£o encontrado: {csv_path}")
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            zip_filename = f"Teste_{your_name}_{timestamp}.zip"
            zip_path = os.path.join(self.output_dir, zip_filename)
            
            with logging_redirect_tqdm():
                with tqdm(total=100, desc="Compactando arquivo") as pbar:
                    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                        zipf.write(csv_path, arcname=os.path.basename(csv_path))
                    pbar.update(100)
            
            log.success(f"Arquivo compactado: {zip_path}")
            return zip_path
        except Exception as e:
            log.error(f"Erro ao compactar arquivo: {str(e)}")
            return None

    def process(self, your_name):
        """Executa todo o fluxo de transforma√ß√£o"""
        log.info("Iniciando processo de transforma√ß√£o de dados")
        
        logging.basicConfig(handlers=[logging.StreamHandler()], level=logging.INFO)
        
        with logging_redirect_tqdm():
            with tqdm(total=4, desc="Processo completo") as main_pbar:
                raw_data = self.extract_tables_from_pdf()
                if raw_data is None:
                    return False
                main_pbar.update(1)
                
                df = self.clean_and_transform_data(raw_data)
                if df is None or df.empty:
                    return False
                main_pbar.update(1)
                
                csv_path = self.save_to_csv(df)
                if csv_path is None:
                    return False
                main_pbar.update(1)
                
                zip_path = self.compress_csv(csv_path, your_name)
                if zip_path is None:
                    return False
                main_pbar.update(1)
                
                return True

if __name__ == "__main__":
    try:
        log.remove()
        log.add(lambda msg: tqdm.write(msg, end=""), colorize=True)
        
        project_root = Path(__file__).parent.parent
        pdf_path = project_root / "web_scraping" / "downloads" / "Anexo_I.pdf"
        
        if not pdf_path.exists():
            log.error(f"Arquivo PDF n√£o encontrado em: {pdf_path}")
            exit(1)
        
        log.info(f"Iniciando processamento do arquivo: {pdf_path}")
        transformer = ANSDataTransformer(str(pdf_path))
        success = transformer.process("ErickFernandesDeFariasSantos")
        
        if success:
            log.info("Processo conclu√≠do com sucesso!")
        else:
            log.error("Ocorreu um erro durante o processamento")
    
    except Exception as e:
        log.error(f"Erro fatal: {str(e)}")
```

```bash
python3 main.p
```

Os dados estavam saindo todos bagun√ßados, da√≠, utilizei a IA para apoiar e buscar uma melhor solu√ß√£o para tal problema.

![image.png](Documentac%CC%A7a%CC%83o%20de%20scripts%20Python%201c51e83b35a080218439f497b3b174e4/image%201.png)

At√© que ficou organizado, se comparado com bagun√ßa que estava.

![image.png](Documentac%CC%A7a%CC%83o%20de%20scripts%20Python%201c51e83b35a080218439f497b3b174e4/image%202.png)

## Baixar arquivos e Banco de dados (PostgreSQL 17)

Arquivos baixados dos 2 √∫ltimos anos e tamb√©m o Relatorio_cadop.cs

![image.png](Documentac%CC%A7a%CC%83o%20de%20scripts%20Python%201c51e83b35a080218439f497b3b174e4/image%203.png)

![image.png](Documentac%CC%A7a%CC%83o%20de%20scripts%20Python%201c51e83b35a080218439f497b3b174e4/image%204.png)

### Cria√ß√£o das tabelas

```sql
CREATE TABLE operadoras (
    registro_ans VARCHAR(20) PRIMARY KEY,
    cnpj VARCHAR(14) NOT NULL,
    razao_social VARCHAR(255) NOT NULL,
    nome_fantasia VARCHAR(255),
    modalidade VARCHAR(50) NOT NULL,
    logradouro VARCHAR(255),
    numero VARCHAR(20),
    complemento VARCHAR(100),
    bairro VARCHAR(100),
    cidade VARCHAR(100),
    uf CHAR(2) CHECK (uf IN ('AC','AL','AP','AM','BA','CE','DF','ES','GO','MA','MT','MS','MG','PA','PB','PR','PE','PI','RJ','RN','RS','RO','RR','SC','SP','SE','TO')),
    cep VARCHAR(8),
    ddd CHAR(2),
    telefone VARCHAR(20),
    fax VARCHAR(20),
    endereco_eletronico VARCHAR(255),
    representante VARCHAR(255),
    cargo_representante VARCHAR(100),
    regiao_comercializacao VARCHAR(100),
    data_registro_ans DATE NOT NULL
);

COMMENT ON TABLE operadoras IS 'Dados cadastrais das operadoras de sa√∫de ativas';

CREATE TABLE demonstracoes_contabeis (
    data DATE NOT NULL,
    reg_ans VARCHAR(20) NOT NULL REFERENCES operadoras(registro_ans),
    cd_conta_contabil VARCHAR(20) NOT NULL,
    descricao VARCHAR(255) NOT NULL,
    vl_saldo_inicial NUMERIC(15,2) NOT NULL,
    vl_saldo_final NUMERIC(15,2) NOT NULL,
    ano INT GENERATED ALWAYS AS (EXTRACT(YEAR FROM data)) STORED,
    mes INT GENERATED ALWAYS AS (EXTRACT(MONTH FROM data)) STORED,
    PRIMARY KEY (data, reg_ans, cd_conta_contabil)
);

CREATE INDEX ON demonstracoes_contabeis(reg_ans);
CREATE INDEX ON demonstracoes_contabeis(data);
```

![image.png](Documentac%CC%A7a%CC%83o%20de%20scripts%20Python%201c51e83b35a080218439f497b3b174e4/image%205.png)

### script

```bash
#!/usr/bin/env bash
# importa_script.sh - Script de importa√ß√£o de dados para PostgreSQL
#
# Site: https://github.com/Erick-Fernandes-dev
# Autor: Erick Farias
# Manuten√ß√£o: Erick Farias
#
# ------------------------------------------------------------------------ #
# Este script realiza a importa√ß√£o de dados para o PostgreSQL a partir de arquivos CSV
# seguindo a estrutura de diret√≥rios pr√©-definida. Os dados incluem:
# - Cadastro de operadoras de sa√∫de ativas
# - Demonstra√ß√µes cont√°beis dos anos de 2023 e 2024
#
# Exemplo de uso:
#     $ ./importa_script.sh
#     Inicia o processo de importa√ß√£o dos dados para o banco PostgreSQL
#
# ------------------------------------------------------------------------ #
# Hist√≥rico:
# 
#    v1.0 31/03/2025, Erick Farias:
#       - Vers√£o inicial do script de importa√ß√£o
#       - Implementa√ß√£o das fun√ß√µes b√°sicas de importa√ß√£o
#
# ------------------------------------------------------------------------ #
# Testado em:
#   - bash 4.4.19
#   - PostgreSQL 13+
#   - Ubuntu 20.04 LTS
#
# ------------------------------------------------------------------------ #

# ------------------------------- VARI√ÅVEIS ----------------------------------------- #
# Configura√ß√µes de diret√≥rios e banco de dados
SCRIPT_DIR=$(dirname "$0")
PROJECT_ROOT=$(cd "$SCRIPT_DIR/../../.." && pwd)
DATA_DIR="${PROJECT_ROOT}/src/database/arquivos"
DB_NAME="my_pgdb"
DB_USER="postgres"

# ------------------------------------------------------------------------ #

# ------------------------------- TESTES ----------------------------------------- #
# Configura√ß√µes de tratamento de erros
set -e  # Encerra execu√ß√£o em caso de erro
set -u  # Verifica vari√°veis n√£o definidas

# ------------------------------------------------------------------------ #

# ------------------------------- FUN√á√ïES ----------------------------------------- #

# Verifica a estrutura de diret√≥rios e arquivos necess√°rios
verify_directories() {
    local required_dirs=(
        "${DATA_DIR}/2023"
        "${DATA_DIR}/2024"
    )
    
    for dir in "${required_dirs[@]}"; do
        if [ ! -d "$dir" ]; then
            echo "Erro: Diret√≥rio n√£o encontrado: $dir"
            exit 1
        fi
    done
    
    if [ ! -f "${DATA_DIR}/operadoras.csv" ]; then
        echo "Erro: Arquivo operadoras.csv n√£o encontrado em ${DATA_DIR}"
        exit 1
    fi
}

# ------------------------------------------------------------------------ #

# ------------------------------- EXECU√á√ÉO ----------------------------------------- #
# Fun√ß√£o principal que orquestra o processo de importa√ß√£o
main() {
    echo "Iniciando importa√ß√£o de dados..."
    echo "Diret√≥rio base: ${PROJECT_ROOT}"
    
    verify_directories

    # Importa dados cadastrais das operadoras
    echo "Importando dados das operadoras..."
    psql -U "${DB_USER}" -d "${DB_NAME}" -c "\copy operadoras FROM '${DATA_DIR}/operadoras.csv' DELIMITER ';' CSV HEADER ENCODING 'UTF8'"

    # Processa arquivos cont√°beis por ano
    for year in 2023 2024; do
        echo "Processando ano: ${year}..."
        for file in "${DATA_DIR}/${year}/"*.csv; do
            if [ -f "$file" ]; then
                echo "Importando: $(basename "$file")"
                psql -U "${DB_USER}" -d "${DB_NAME}" -c "\copy demonstracoes_contabeis(data, reg_ans, cd_conta_contabil, descricao, vl_saldo_inicial, vl_saldo_final) FROM '${file}' DELIMITER ';' CSV HEADER ENCODING 'UTF8'"
            else
                echo "Aviso: Nenhum arquivo CSV encontrado em ${DATA_DIR}/${year}"
            fi
        done
    done

    echo "Importa√ß√£o conclu√≠da com sucesso!"
}

# Inicia execu√ß√£o do script
main

# ------------------------------------------------------------------------ #
```

Esse padr√£o de coment√°rio que est√° nesse Script, aprendi em um urso de Shellscript.

Vai dar um erro de `Peer authentication failed for user "postgres"`, 

## Solu√ß√£o (sugerida pela IA)

O erro `Peer authentication failed for user "postgres"` ocorre porque o PostgreSQL est√° configurado para usar o m√©todo de autentica√ß√£o `peer` para o usu√°rio `postgres`. Esse m√©todo exige que o nome do usu√°rio do sistema operacional corresponda ao nome do usu√°rio do banco de dados. Como voc√™ est√° tentando usar o usu√°rio `postgres` com uma senha, √© necess√°rio alterar o m√©todo de autentica√ß√£o para `md5` ou `password`.

### Solu√ß√£o

1. **Edite o arquivo de configura√ß√£o do PostgreSQL**:
Abra o arquivo `pg_hba.conf` para editar as configura√ß√µes de autentica√ß√£o. Normalmente, ele est√° localizado em `/etc/postgresql/<vers√£o>/main/pg_hba.conf`.
    
    ```bash
    sudo nano /etc/postgresql/17/main/pg_hba.conf
    
    ```
    
2. **Altere o m√©todo de autentica√ß√£o para `md5`**:
Localize a linha que cont√©m `local all postgres peer` e altere `peer` para `md5`:
    
    ```
    local   all   postgres   md5
    
    ```
    
3. **Reinicie o PostgreSQL**:
Ap√≥s salvar as altera√ß√µes, reinicie o servi√ßo do PostgreSQL para aplicar as mudan√ßas:
    
    ```bash
    sudo systemctl restart postgresql
    
    ```
    
4. **Atualize o script para incluir a senha**:
No seu script importa_script.sh, adicione a vari√°vel de ambiente `PGPASSWORD` para passar a senha automaticamente:
    
    ```bash
    export PGPASSWORD="tbwa0002"
    
    ```
    
    Atualize a linha de importa√ß√£o no script para:
    
    ```bash
    PGPASSWORD="tbwa0002" psql -U "${DB_USER}" -d "${DB_NAME}" -c "\\copy operadoras FROM '${DATA_DIR}/operadoras.csv' DELIMITER ';' CSV HEADER ENCODING 'UTF8'"
    
    ```
    
5. **Execute o script novamente**:
Agora, execute o script normalmente:
    
    ```bash
    ./importa_script.sh
    
    ```
    

### Queries anal√≠ticas

```sql
-- 10 maiores despesas no √∫ltimo trimestre completo
WITH ultimo_trimestre AS (
    SELECT 
        date_trunc('quarter', CURRENT_DATE) - INTERVAL '3 months' AS inicio,
        date_trunc('quarter', CURRENT_DATE) - INTERVAL '1 day' AS fim
)
SELECT 
    o.razao_social,
    SUM(d.vl_saldo_final - d.vl_saldo_inicial) AS total_despesas
FROM demonstracoes_contabeis d
JOIN operadoras o ON d.reg_ans = o.registro_ans
WHERE d.descricao ILIKE 'EVENTOS/ SINISTROS CONHECIDOS OU AVISADOS DE ASSIST√äNCIA A SA√öDE MEDICO HOSPITALAR'
AND d.data BETWEEN (SELECT inicio FROM ultimo_trimestre) 
                AND (SELECT fim FROM ultimo_trimestre)
GROUP BY o.razao_social
ORDER BY total_despesas DESC
LIMIT 10;

-- 10 maiores despesas no √∫ltimo ano
SELECT 
    o.razao_social,
    SUM(d.vl_saldo_final - d.vl_saldo_inicial) AS total_despesas
FROM demonstracoes_contabeis d
JOIN operadoras o ON d.reg_ans = o.registro_ans
WHERE d.descricao ILIKE 'EVENTOS/ SINISTROS CONHECIDOS OU AVISADOS DE ASSIST√äNCIA A SA√öDE MEDICO HOSPITALAR'
AND d.data BETWEEN (CURRENT_DATE - INTERVAL '1 year') AND CURRENT_DATE
GROUP BY o.razao_social
ORDER BY total_despesas DESC
LIMIT 10;
```

## Aplica√ß√£o com Vue.js e Python

![image.png](Documentac%CC%A7a%CC%83o%20de%20scripts%20Python%201c51e83b35a080218439f497b3b174e4/image%206.png)

## **Faltou alguns detalhes para executar o Vue.js.**

## **OBS! N√£o manjo muito de Front-end, mas se caso for selecionado para a vaga, farei de tudo para aprender. üë®üèª‚Äçüíª**

### Vou mostrar at√© onde consegui

Consegui implementar apenas o Backend, onde desenvolvi um script em 
Python para realizar a conex√£o com o PostgreSQL. No script, configurei 
os par√¢metros necess√°rios para estabelecer a comunica√ß√£o com o banco de 
dados. Em seguida, utilizei um arquivo CSV como fonte de dados - defini o
 caminho at√© o diret√≥rio *resources* onde o arquivo est√° localizado. O script √© capaz de ler os dados contidos no CSV e inseri-los corretamente no banco de dados.

![image.png](Documentac%CC%A7a%CC%83o%20de%20scripts%20Python%201c51e83b35a080218439f497b3b174e4/image%207.png)

Dados do **`operadores.csv`** implementados para dentro do meu PostgreSQL na vers√£o 17.

![image.png](Documentac%CC%A7a%CC%83o%20de%20scripts%20Python%201c51e83b35a080218439f497b3b174e4/image%208.png)

![image.png](Documentac%CC%A7a%CC%83o%20de%20scripts%20Python%201c51e83b35a080218439f497b3b174e4/image%209.png)

Para executar v√° at√© o diret√≥rio python e execcute:

```bash
uvicorn app.main:app --reload --port 8010
```

### Libs:

```bash
pip install uvicorn fastapi sqlalchemy csv pathlib
```

### Estutura de diret√≥rio de Frontend

![image.png](Documentac%CC%A7a%CC%83o%20de%20scripts%20Python%201c51e83b35a080218439f497b3b174e4/image%2010.png)

## Conclus√£o

Foi uma experi√™ncia muito enriquecedora implementar meus conhecimentos nesses desafios. Embora o projeto tenha sido funcional, h√° diversas melhorias que poderiam ser implementadas, como a ado√ß√£o de padr√µes de projeto para organizar melhor o c√≥digo, a cria√ß√£o de uma infraestrutura robusta utilizando Docker e Kubernetes, al√©m da implementa√ß√£o de testes unit√°rios e automatizados para garantir a qualidade do software.

Durante o desenvolvimento, a resolu√ß√£o de bugs foi um processo interessante, onde utilizei ferramentas como IA, Stack Overflow e a documenta√ß√£o oficial das bibliotecas para encontrar solu√ß√µes e aprender mais sobre as tecnologias envolvidas. Foi uma √≥tima oportunidade para consolidar conhecimentos e explorar novas abordagens.
